{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dd1805-25a0-4236-ba3b-167ebf134a7d",
   "metadata": {},
   "source": [
    "# Overview of PySpark data management\n",
    "\n",
    "In this notebook, we will illustrate how various data verbs are implemented in `pyspark`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699b982-cd1d-4cfa-8e5d-2b50c2cc4a7f",
   "metadata": {},
   "source": [
    "## `polars` $\\approx$ `pyspark`\n",
    "\n",
    "You will see a lot of similarities between `polars` and `pyspark`\n",
    "\n",
    "1. Lazy evaluation and column expression,\n",
    "2. Parallel processing out-of-the-box,\n",
    "3. Dot-chained queries, and\n",
    "4. Data verbs related to `SQL` and/or `dplyr`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6016f",
   "metadata": {
    "id": "udyGCYQJUeA7"
   },
   "source": [
    "## Data verbs in `pyspark`\n",
    "\n",
    "In this lecture, we will look at how the common data verbs are implemented in `pyspark`.  Luckily, the implementation is similar to `polars`, so it should be a relatively pain-free transition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de2e01",
   "metadata": {},
   "source": [
    "### Overview of Basic Data Verbs in `polars` and `pyspark`\n",
    "\n",
    "Verb/Function | `polars` | `pyspark` |\n",
    "--------------|----------|-----------|\n",
    "Column expr.  | `pl.col('name') ...` | `col('name') ...`|\n",
    "SELECT | `.select(...)` | `.select(...)` |\n",
    "FILTER | `.filter(...)` | `.where(...)` |\n",
    "MUTATE | `.with_columns(...)` | `.withColumn(...)` |\n",
    "GROUPBY | `.group_by(...)` | `.groupBy(...)`|\n",
    "AGGREGATE | `.agg(...)` | `.agg(...)` |\n",
    "JOIN | `l_tbl.join(r_tbl,...)` | `l_tbl.join(r_tbl,...)`|\n",
    "UNION | `pl.concat` or SQL | `t1.union(t2)` | \n",
    "STACK COLUMNS | `.unpivot(...)` | `.unpivot(...)`|\n",
    "UNSTACK COLUMNS | `.pivot(...)` | `.groupBy(...).pivot(...).<aggfunc>(...)`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be9f27",
   "metadata": {
    "id": "i2X_PXZaUeA-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean\n",
    "\n",
    "spark = SparkSession.builder.appName('Ops').getOrCreate()\n",
    "heroes = spark.read.csv('./data/heroes_information.csv', inferSchema=True, header=True)\n",
    "\n",
    "heroes.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caba70d",
   "metadata": {
    "id": "6DJmNr8MUeA_"
   },
   "source": [
    "## Selecting Columns\n",
    "\n",
    "The first verb, `select` \n",
    "\n",
    "* filters the *columns*\n",
    "* At the core of `SQL` statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aea626",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Cx2vQRhXUeBA",
    "outputId": "478aefe5-3b12-460f-f199-87b26b6c1e0d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "(select_query :=\n",
    " heroes\n",
    " .select(heroes.name,      # Column via dataframe.name\n",
    "         col('Gender'),    # Column expression (lazy)\n",
    "         'Weight')         # String\n",
    ").limit(5).toPandas()      # <-- outside the saved query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81519c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_query  # <-- lazy query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c991a2",
   "metadata": {
    "id": "92gaGoxLUeBB"
   },
   "source": [
    "## Filtering Rows\n",
    "\n",
    "The next verb, `filter` \n",
    "\n",
    "* filters the *rows*\n",
    "* is related to the `SQL` `WHERE` clause\n",
    "* `pyspark`: Use the `where` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e8747",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbxipZCRUeBD",
    "outputId": "bdbbb794-2344-4fc0-d2dd-58bbb17e10a7"
   },
   "outputs": [],
   "source": [
    "col('Gender') == 'Male' # <-- Lazy column expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88c030",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "9nylNFA3UeBC",
    "outputId": "29b10db1-187f-4571-9e05-5b6fdd9e2994"
   },
   "outputs": [],
   "source": [
    "(heroes\n",
    " .where(col('Gender') == 'Male')\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe9237",
   "metadata": {
    "id": "6HgVUjfEUeBF"
   },
   "source": [
    "## Chaining Data Verbs\n",
    "\n",
    "* Processing df $\\rightarrow$ chaining data verbs\n",
    "* Accomplished through dot-chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340c118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "srKXbDV6UeBG",
    "outputId": "c1f9813f-0047-47ec-9728-d0b691ac0e80",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(heroes\n",
    " .where(col('Gender') == 'Male')\n",
    " .select('name', \n",
    "         'Gender', \n",
    "         'Weight')\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec899f4",
   "metadata": {
    "id": "Uxh6S9eWUeBJ"
   },
   "source": [
    "## Constructing New Columns\n",
    "\n",
    "The third verb, `mutate` \n",
    "\n",
    "* Creates new columns\n",
    "* Changes existing columns\n",
    "* `pyspark`: Use the `withColumns` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725a36b",
   "metadata": {
    "id": "p2ISjtoDUeBJ"
   },
   "source": [
    "### Example 3 - Converting Weight to kilograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0616f4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "7j_QUg66UeBL",
    "outputId": "8de7ac1b-d530-455f-92d1-9b742c92b61b"
   },
   "outputs": [],
   "source": [
    "(heroes\n",
    " .select('name', \n",
    "         'Gender', \n",
    "         'Weight')\n",
    " .withColumn('Weight_kg', col('Weight')/2.2046)\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2238db",
   "metadata": {
    "id": "DhZ7KfmSUeBM"
   },
   "source": [
    "## Referencing a new column\n",
    "\n",
    " Use the `col` function with the label from `withColumn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2048c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "7E3WFI6iUeBN",
    "outputId": "c5f04301-d3af-484e-a82c-d9783db9b2ac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(new_col_result := \n",
    " heroes\n",
    " .select('name', \n",
    "         'Gender', \n",
    "         'Weight')\n",
    " .withColumn('Weight_kg', col('Weight')/2.2046)\n",
    " .where(col('Weight_kg') < 100)  # <-- one reason we need lazy expressions\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafd49b",
   "metadata": {
    "id": "Z4PJOzNjVjl2"
   },
   "source": [
    "## Simple and Grouped Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5ff99",
   "metadata": {
    "id": "VtpXCInFVjl8"
   },
   "outputs": [],
   "source": [
    "(pitching :=  \n",
    " spark.read.csv('./data/Pitching.csv', inferSchema=True, header=True)\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de843d",
   "metadata": {
    "id": "5qvXkU8MVjmA"
   },
   "source": [
    "### Simple Aggregation\n",
    "\n",
    "A **simple aggregation** collapses all rows into one row.\n",
    "\n",
    "<img src=\"https://github.com/wsu-stat489/module5_intro_to_pyspark/blob/main/img/simple_aggregation.png?raw=1\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49276d81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "gdkY5XXiVjmC",
    "outputId": "a056f593-46ec-4772-bdea-a9b8339f85b5"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, std, max, min\n",
    "\n",
    "(pitching\n",
    "  .agg(mean('ERA').alias('mean_era'),\n",
    "       std('ERA').alias('sd_era'),\n",
    "       max('W').alias('max_wins'),\n",
    "       min('W').alias('min_wins'))\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934adafc",
   "metadata": {
    "id": "OrpgA4E-VjmG"
   },
   "source": [
    "### Group and Aggregate\n",
    "\n",
    "<img src=\"https://github.com/wsu-stat489/module5_intro_to_pyspark/blob/main/img/group_and_aggregate.png?raw=1\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad89c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "4BieFi85VjmH",
    "outputId": "05e4a9ca-ac3b-49d2-9fdf-41251dc68dc5"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "(eras := \n",
    " pitching\n",
    " .where((col('yearID') >= 1900) & (col('yearID') < 1940)) \n",
    " .withColumn('era', (when(col('yearID') < 1920, \"dead ball\") \n",
    "                     .otherwise(\"after dead ball\" ) \n",
    "                    )\n",
    "            )\n",
    " .groupby('era')\n",
    " .agg(mean('R').alias('mean_runs'))\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d0bab2",
   "metadata": {
    "id": "yTZ1sqUeVjmL"
   },
   "source": [
    "### Grouping by more than one category\n",
    "\n",
    "* `group_by` accepts multiple columns\n",
    "* Groups all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad2866",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "j9ZX0bIdVjmM",
    "outputId": "310c37f6-c293-48dc-a22d-4fd1a2dc5eaf"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "(pitching\n",
    " .select('yearID', 'teamID', 'W')\n",
    " .where(col('yearID') >= 1946)\n",
    " .groupby('yearID', 'teamID')\n",
    " .agg(sum('W').alias('total_wins'))\n",
    " .where(col('total_wins') >= 100)\n",
    " .sort(col('yearID').asc(), col('total_wins').desc())\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480f724",
   "metadata": {
    "id": "sCZY7My-WsG_"
   },
   "source": [
    "## Joins in `pyspark`\n",
    "\n",
    "Performed with `df_left.join(df_right, how=type_str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0740e47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "N6O7UrO-WsHF",
    "outputId": "7bed4c49-dbed-4e55-a137-b09cc1720d45"
   },
   "outputs": [],
   "source": [
    "(dept := \n",
    " spark.read.csv(\"./data/department.csv\",  header=True, inferSchema=True)\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fbd0fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "5kS1jQohWsHI",
    "outputId": "2622fa48-1e7f-4f94-e2d4-bc690d08a05d"
   },
   "outputs": [],
   "source": [
    "(empl := \n",
    " spark.read.csv(\"./data/employee.csv\",  header=True, inferSchema=True)\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5e910",
   "metadata": {
    "id": "wJoYU2H-WsHJ"
   },
   "source": [
    "#### Inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170182a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "kHWnfWe7WsHJ",
    "outputId": "b787b789-7c3b-4c4c-dc50-95c1d576968d"
   },
   "outputs": [],
   "source": [
    "(empl.join(dept, empl.DeptID == dept.DeptID, how='inner')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6f1c3",
   "metadata": {
    "id": "WWbFvkGNWsHK"
   },
   "source": [
    "#### Left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ec5af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "mLOscmieWsHK",
    "outputId": "c9d09c1e-b092-4475-c1b0-e491131f02c8"
   },
   "outputs": [],
   "source": [
    "(empl.join(dept, empl.DeptID == dept.DeptID, how='left')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23805b80",
   "metadata": {
    "id": "X24B3LiRWsHL"
   },
   "source": [
    "#### Right join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e95804",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "7yt7DldlWsHL",
    "outputId": "1800d304-b8dd-4858-f868-35da5be3e08c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(empl.join(dept, empl.DeptID == dept.DeptID, how='right')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737ea1e",
   "metadata": {
    "id": "LCFMCHXKWsHM"
   },
   "source": [
    "#### Outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efcbf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "R17T8Uq-WsHN",
    "outputId": "d3a3484d-87a0-46b1-ebed-6edfb4d6d6b2"
   },
   "outputs": [],
   "source": [
    "(empl.join(dept, empl.DeptID == dept.DeptID, how='outer')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff241bc8",
   "metadata": {
    "id": "r8ax1-rgWsHN"
   },
   "source": [
    "## Joining on multiple keys\n",
    "\n",
    "Next, we will look at table joins that require matching multiple keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8bde1",
   "metadata": {
    "id": "TgbF4zZjWsHO"
   },
   "source": [
    "### Example -- Total At Bats, Hits, and Runs Allowed in 2010\n",
    "\n",
    "To illustrate joining on multiple keys, lets\n",
    "\n",
    "1. Compute the totals for H and R in 2010 for each team from the `Pitching` table.\n",
    "2. Join on the team name and park.\n",
    "\n",
    "This is a good example, because team information can change over the years, so we need to match both `teamID` and `yearID`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e519f32",
   "metadata": {
    "id": "fZenmGJOWsHP"
   },
   "source": [
    "#### Step 1. Read and process the pitching table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdd684",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "3CyMumvaWsHP",
    "outputId": "a4b3c190-ae79-4384-816c-31c601c83872"
   },
   "outputs": [],
   "source": [
    "(pitching := \n",
    " spark.read.csv(\"./data/Pitching.csv\", header=True, inferSchema=True)\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "(teams := \n",
    " spark.read.csv(\"./data/Teams.csv\", header=True, inferSchema=True)\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ad4f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "WU2wRHweWsHQ",
    "outputId": "9fc2cf9a-b214-4ec2-c183-ba4b1e07d572"
   },
   "outputs": [],
   "source": [
    "(pitching_totals_2010 := \n",
    " pitching\n",
    " .select('teamID', 'yearID', 'R', 'H')\n",
    " #.where(col('yearID') == 2010)\n",
    " #.groupBy('teamID', 'yearID')\n",
    " #.agg(sum('R').alias('Total Runs'), \n",
    "     # sum('H').alias('Total Hits'))\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab55ea0",
   "metadata": {
    "id": "8KThZR_XWsHR"
   },
   "source": [
    "#### Step 2. Read and process the teams table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6041d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "1raZFaEuWsHR",
    "outputId": "c5b30afe-1424-4a48-ee9f-6744a5fe9aa7"
   },
   "outputs": [],
   "source": [
    "(team_name_and_park := \n",
    " teams\n",
    " .select('yearID', 'teamID', col('name').alias('Team Name'), 'park')\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38738591",
   "metadata": {
    "id": "2AXpf3ufWsHS"
   },
   "source": [
    "#### Step 3. Perform a left-join.\n",
    "\n",
    "Since we want to keep all rows in the totals table, and only add the team information when available, we will perform a left join on the totals table.\n",
    "\n",
    "Notice that the second `on` argument is now a `list` of column expressions, one for each matching rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543d010",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "oe2fGbX7WsHS",
    "outputId": "79954521-a770-4a03-cba0-a347c4143cb3"
   },
   "outputs": [],
   "source": [
    "(pitching_totals_2010\n",
    " .join(team_name_and_park,\n",
    "       on = [pitching_totals_2010.yearID == team_name_and_park.yearID,\n",
    "             pitching_totals_2010.teamID == team_name_and_park.teamID],\n",
    "       how='left')\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a877a10",
   "metadata": {},
   "source": [
    "## Concatenating Tables with Set-Like Operations in `pyspark`\n",
    "\n",
    "Now let's look at combining tables with `union`, `intersect`, and `except` in `pyspark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4aa225",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sales_apr := \n",
    " spark.read.csv(\"./data/auto_sales_apr.csv\",  header=True, inferSchema=True)\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d118e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sales_may := \n",
    " spark.read.csv(\"./data/auto_sales_may.csv\",  header=True, inferSchema=True)\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bb405",
   "metadata": {},
   "source": [
    "#### UNION and UNION DISTINCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cef9c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(combined_sales :=\n",
    " sales_apr\n",
    " .union(sales_may)\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df70b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sales_apr\n",
    " .union(sales_may)\n",
    " .distinct()\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61433797",
   "metadata": {},
   "source": [
    "#### Including information from the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "(combined_sales :=\n",
    " sales_apr\n",
    " .drop('ID')\n",
    " .withColumn('Month', lit('Apr'))  # <-- use `lit` to provide a Java literal (similar to pl.lit in polars)\n",
    " .union(sales_may\n",
    "        .drop('ID')\n",
    "        .withColumn('Month', lit('May'))\n",
    "       )\n",
    ").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed02f3",
   "metadata": {},
   "source": [
    "#### INTERSECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce15335",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sales_apr\n",
    " .intersect(sales_may)\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f241a",
   "metadata": {},
   "source": [
    "#### DIFFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sales_apr\n",
    " .exceptAll(sales_may)\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e04a6",
   "metadata": {},
   "source": [
    "## Reshaping tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3b0c8",
   "metadata": {},
   "source": [
    "#### Stacking columns with `unpivot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45806d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(combined_sales\n",
    " .unpivot(ids = ['Salesperson', 'Month'],\n",
    "          values = ['Compact','Sedan','SUV','Truck'],\n",
    "          variableColumnName='Type',\n",
    "          valueColumnName='Sales'\n",
    "         )\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9753a34",
   "metadata": {},
   "source": [
    "#### Unstacking columns with GROUPBY + PIVOT + SUMMARY METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(combined_sales\n",
    " .unpivot(ids = ['Salesperson', 'Month'],\n",
    "          values = ['Compact','Sedan','SUV','Truck'],\n",
    "          variableColumnName='Type',\n",
    "          valueColumnName='Sales'\n",
    "         )\n",
    " .groupBy('Salesperson')\n",
    " .pivot('Type')\n",
    " .sum('Sales')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713c586",
   "metadata": {},
   "source": [
    "## Review of Basic Data Verbs in `polars` and `pyspark`\n",
    "\n",
    "Verb/Function | `polars` | `pyspark` |\n",
    "--------------|----------|-----------|\n",
    "Column expr.  | `pl.col('name') ...` | `col('name') ...`|\n",
    "SELECT | `.select(...)` | `.select(...)` |\n",
    "FILTER | `.filter(...)` | `.where(...)` |\n",
    "MUTATE | `.with_columns(...)` | `.withColumn(...)` |\n",
    "GROUPBY | `.group_by(...)` | `.groupBy(...)`|\n",
    "AGGREGATE | `.agg(...)` | `.agg(...)` |\n",
    "JOIN | `l_tbl.join(r_tbl,...)` | `l_tbl.join(r_tbl,...)`|\n",
    "UNION | `pl.concat` or SQL | `t1.union(t2)` | \n",
    "STACK COLUMNS | `.unpivot(...)` | `.unpivot(...)`|\n",
    "UNSTACK COLUMNS | `.pivot(...)` | `.groupBy(...).pivot(...).<aggfunc>(...)`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f2af9b",
   "metadata": {
    "id": "J-HPkqBJWsHT"
   },
   "source": [
    "## <font color=\"red\"> Exercise 4.3 </font>\n",
    "\n",
    "Determine all the players that have hit more than 40 home runs in a season in the modern era (e.g., since 1946).  The final table should include the players proper name, as well as the team name. \n",
    "\n",
    "**Tasks.**\n",
    "\n",
    "1. Select and filter where possible,\n",
    "2. Be sure to aggregate across the stints to compute total HR for each player-year,\n",
    "3. Remove and keys after joining proper names, and\n",
    "4. Sort the results by year (outer; ascending) and total HR (inner; descending)\n",
    "\n",
    "**Hint:** You will need join the files listed below.  To get credit for this exercise, use the join `pyspark` join methods presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4fa79",
   "metadata": {
    "id": "H-HPCeUMWsHU"
   },
   "outputs": [],
   "source": [
    "(teams := \n",
    " spark.read.csv(\"./data/teams.csv\",  header=True, inferSchema=True)\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "(batting := \n",
    " spark.read.csv(\"./data/batting.csv\",  header=True, inferSchema=True)\n",
    ").toPandas().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "(people :=\n",
    " spark.read.csv('./data/People.csv', header = True, inferSchema=True)\n",
    ").toPandas().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(batting_teams :=\n",
    " batting.join(teams, batting.yearID == teams.yearID, how='inner')\n",
    ").limit(5).toPandas().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "266097b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yearID</th>\n",
       "      <th>nameFirst</th>\n",
       "      <th>nameLast</th>\n",
       "      <th>total_HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947</td>\n",
       "      <td>Ralph</td>\n",
       "      <td>Kiner</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1947</td>\n",
       "      <td>Johnny</td>\n",
       "      <td>Mize</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949</td>\n",
       "      <td>Ralph</td>\n",
       "      <td>Kiner</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949</td>\n",
       "      <td>Ted</td>\n",
       "      <td>Williams</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1950</td>\n",
       "      <td>Ralph</td>\n",
       "      <td>Kiner</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>2019</td>\n",
       "      <td>Christian</td>\n",
       "      <td>Yelich</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2019</td>\n",
       "      <td>Alex</td>\n",
       "      <td>Bregman</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2019</td>\n",
       "      <td>Nelson</td>\n",
       "      <td>Cruz</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2019</td>\n",
       "      <td>Nolan</td>\n",
       "      <td>Arenado</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2019</td>\n",
       "      <td>Ronald</td>\n",
       "      <td>Acuna</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     yearID  nameFirst  nameLast  total_HR\n",
       "0      1947      Ralph     Kiner        51\n",
       "1      1947     Johnny      Mize        51\n",
       "2      1949      Ralph     Kiner        54\n",
       "3      1949        Ted  Williams        43\n",
       "4      1950      Ralph     Kiner        47\n",
       "..      ...        ...       ...       ...\n",
       "255    2019  Christian    Yelich        44\n",
       "256    2019       Alex   Bregman        41\n",
       "257    2019     Nelson      Cruz        41\n",
       "258    2019      Nolan   Arenado        41\n",
       "259    2019     Ronald     Acuna        41\n",
       "\n",
       "[260 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(forty_homers :=\n",
    " batting\n",
    "  .select('playerID','yearID', 'HR', 'teamID')\n",
    "  .where(col('yearID') > 1946)\n",
    "  .groupBy('playerID', 'yearID')\n",
    "  .agg(sum('HR').alias('total_HR'))\n",
    "  .where(col('total_HR') > 40)\n",
    "  .join(people, batting.playerID == people.playerID, \"inner\")\n",
    "  .drop('playerID')\n",
    "  .join(teams,\n",
    "       on = batting.yearID == teams.yearID,\n",
    "       how='inner')\n",
    "  .select(batting.yearID, 'nameFirst', 'nameLast','total_HR')\n",
    "  .distinct()\n",
    "  .sort(col('yearID').asc(), col('total_HR').desc())\n",
    ").toPandas()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
